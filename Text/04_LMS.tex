\section{Theory}

\subsection{Least Mean Squares}

\newacronym{LMS}{LMS}{Least Mean Squares}

The linear \gls{LMS}, also known as linear regression \parencite{2018StephenBoydIntroduction} is a data fitting technique. % TODO

We present here the theory for real numbers, noting that it can be easily extended for complex numbers \parencite{1995LawsonSolving}, should the necessity arise. That, however, will not be the case in this work.

Given two vectors $ X $ and $ Y $, such that:
$$ X = [x_0, x_1, \cdots ,x_{m-1}]^T, \ x_i \ \in \ \mathbb{R} \ \forall \ i, \ 0 \le i < m \in \mathbb{N} $$
$$ Y = [y_0, y_1, \cdots ,y_{m-1}]^T, \ y_i \ \in \ \mathbb{R} \ \forall \ i, \ 0 \le i < m \in \mathbb{N} $$
We ideally want to find the vector of coefficients $ A = [a_0, a_1, \cdots ,a_k]^T, \ a_j \ \in \ \mathbb{R} \ \forall \ j, \ 0 \le j \le k \in \mathbb{N} $ that satisfies the equality $ x^0_i a_0 + x^1_i a_1 + \cdots + x^{k-1}_i a_{k-1} + x^k_i a_k = y_i \quad \forall \quad i \in \{0, \cdots , m-1\} $.

Put in that way, what we have is a system of linear equations that can be rewritten in matrix form as seen in equation \ref{eq:lms}.

\begin{equation}
  \label{eq:lms}
\stackrel{Q_{m \times (k+1)}}
{
\begin{bmatrix}
1 & x_0 & \cdots & x_0^k \\
1 & x_1 & \cdots & x_1^k \\
\vdots & \vdots & \ddots & \vdots  \\
1 & x_{m-1} & \cdots & x_{m-1} \\
\end{bmatrix}
}
\stackrel{A_{(k+1) \times 1}}
{
\begin{bmatrix}
a_0 \\
a_1 \\
\vdots  \\
a_k \\
\end{bmatrix}
}
=
\stackrel{Y_{m \times 1}}
{
\begin{bmatrix}
y_0 \\
y_1 \\
\vdots  \\
y_{m-1} \\
\end{bmatrix}
}
\end{equation}

More compactly, we have that $ Q A = Y $. This system will generally be over-determined, only having an exact solution in the case where $ Y $ is a linear combination of the columns in $ Q $ \parencite{2018StephenBoydIntroduction}.

Since an exact solution is often beyond reach, we can at least choose $ A $ in a way that makes $ Q A $ and $ Y $ close. A way to do that is to minimize the absolute errors (or their square) between those two quantities; $ \underset{A}{\min} ||Q A - Y||^2_2 $

Using calculus, for example, on can arrive at the equation $ \hat{A} = (Q^T Q)^{-1} Q^T Y $, where $ \hat{A} $ is the vector of coefficients that minimizes the squared errors. A detailed derivation is available at \cite{2018StephenBoydIntroduction}.



\subsection{Constrained Least Mean Squares}

\newacronym{CLMS}{CLMS}{Constrained Least Mean Squares}

In the constrained version of the \gls{LMS}, known as the \gls{CLMS} problem we are still interested in finding the coefficient vector $ \hat{A} $ that minimizes $ ||Q A - Y||^2_2 $, but respecting a restriction on this vector of the general form of $ V A = B $.

We can then define the Lagrangian and set its derivatives to zero to obtain, as described in \textcite{2013SelesnickLeast}, the closed form solution 

$$ \hat{A} = \left(Q^{T} Q\right)^{-1} \left(Q^{T} Y - V^{T} \left(V \left(Q^{T} Q\right)^{-1} V^{T}\right)^{-1} V \left(Q^{T} Q\right)^{-1} Q^{T} Y - B \right) $$