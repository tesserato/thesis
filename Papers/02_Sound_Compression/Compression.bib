% Encoding: UTF-8

@InBook{2018BritanakAudio,
  author     = {Britanak, Vladimir and Rao, K. R.},
  booktitle  = {Cosine-/Sine-Modulated Filter Banks},
  date       = {2018},
  title      = {Audio Coding Standards, (Proprietary) Audio Compression Algorithms, and Broadcasting/Speech/Data Communication Codecs: Overview of Adopted Filter Banks},
  bookauthor = {Britanak, Vladimir and Rao, K. R.},
  doi        = {10.1007/978-3-319-61080-1_2},
  isbn       = {978-3-319-61078-8 978-3-319-61080-1},
  location   = {Cham},
  pages      = {13--37},
  publisher  = {Springer International Publishing},
  url        = {http://link.springer.com/10.1007/978-3-319-61080-1_2},
  urldate    = {2021-03-03},
  file       = {2018 Audio Coding Standards, (Proprietary) Audio Compression Algorithms, and BroadcastingSpeechData Communication Codecs Overview of Adopted Filter Banks - Britanak and Rao.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\2018 Audio Coding Standards, (Proprietary) Audio Compression Algorithms, and BroadcastingSpeechData Communication Codecs Overview of Adopted Filter Banks - Britanak and Rao.pdf:application/pdf},
  keywords   = {relevant},
  langid     = {english},
  relevance  = {relevant},
  shorttitle = {Audio Coding Standards, (Proprietary) Audio Compression Algorithms, and Broadcasting/Speech/Data Communication Codecs},
}

@InProceedings{2018WilkinsVocalset,
  author     = {Wilkins, Julia and Seetharaman, Prem and Wahl, Alison and Pardo, Bryan},
  booktitle  = {19th International Society for Music Information Retrieval Conference, Paris,France, 2018},
  date       = {2018-03-08},
  title      = {Vocalset: A Singing Voice Dataset},
  doi        = {10.5281/ZENODO.1203819},
  note       = {type: dataset},
  publisher  = {Zenodo},
  url        = {https://zenodo.org/record/1203819},
  urldate    = {2021-03-03},
  abstract   = {We present {VocalSet}, a singing voice dataset of a capella singing. Existing singing voice datasets either do not capture a large range of vocal techniques, have very few singers, or are single-pitch and devoid of musical context. {VocalSet} captures not only a range of vowels, but also a diverse set of voices on many different vocal techniques, sung in contexts of scales, arpeggios, long tones, and excerpts. {VocalSet} has recordings of 10.1 hours of 20 professional singers (11 male, 9 female) performing 17 different different vocal techniques. This data will facilitate the development of new machine learning models for singer identiﬁcation, vocal technique identiﬁcation, singing generation and other related applications. To illustrate this, we establish baseline results on vocal technique classiﬁcation and singer identiﬁcation by training convolutional network classiﬁers on {VocalSet} to perform these tasks.},
  copyright  = {Creative Commons Attribution 4.0},
  file       = {2018 Vocalset A Singing Voice Dataset - Wilkins et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\2018 Vocalset A Singing Voice Dataset - Wilkins et al..pdf:application/pdf},
  keywords   = {singing, voice, singing dataset, music information retrieval, vocal technique, vowel classification, sung voice},
  langid     = {english},
  rights     = {Creative Commons Attribution 4.0, Open Access},
  shorttitle = {Vocalset},
  year       = {2018},
}

@Book{2001Raotransform,
  author    = {Rao, K},
  date      = {2001},
  title     = {The transform and data compression handbook},
  editor    = {Rao, K. Ramamohan and Yip, P. C.},
  isbn      = {0-8493-3692-9},
  location  = {Boca Raton, Fla},
  pagetotal = {388},
  publisher = {{CRC} Press},
  series    = {The electrical engineering and signal processing series},
  address   = {Boca Raton, Fla},
  file      = {2001 The transform and data compression handbook - Rao and Yip.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\2001 The transform and data compression handbook - Rao and Yip.pdf:application/pdf},
  langid    = {english},
  year      = {2001},
}

@InProceedings{2016ChenCompressing,
  author     = {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian Q. and Chen, Yixin},
  booktitle  = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
  date       = {2016-08-13},
  title      = {Compressing Convolutional Neural Networks in the Frequency Domain},
  doi        = {10.1145/2939672.2939839},
  eventtitle = {{KDD} '16: The 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
  isbn       = {978-1-4503-4232-2},
  location   = {San Francisco California {USA}},
  pages      = {1475--1484},
  publisher  = {{ACM}},
  url        = {https://dl.acm.org/doi/10.1145/2939672.2939839},
  urldate    = {2021-03-03},
  abstract   = {Convolutional neural networks ({CNN}) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to “absorb” great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classiﬁers, hindering many applications such as image and speech recognition on mobile phones and other devices. In this paper, we present a novel network architecture, Frequency-Sensitive Hashed Nets ({FreshNets}), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional ﬁlters are typically smooth and low-frequency, we ﬁrst convert ﬁlter weights to the frequency domain with a discrete cosine transform ({DCT}) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard backpropagation. To further reduce model size, we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate {FreshNets} on eight data sets, and show that it leads to better compressed performance than several relevant baselines.},
  file       = {2016 Compressing Convolutional Neural Networks in the Frequency Domain - Chen et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\2016 Compressing Convolutional Neural Networks in the Frequency Domain - Chen et al..pdf:application/pdf},
  keywords   = {relevant},
  langid     = {english},
  relevance  = {relevant},
}

@Article{2017FitriyaReview,
  author       = {Fitriya, Luluk Anjar and Purboyo, Tito Waluyo and Prasasti, Anggunmeka Luhur},
  date         = {2017},
  journaltitle = {International Journal of Applied Engineering Research},
  title        = {A Review of Data Compression Techniques},
  issn         = {0973-4562},
  number       = {19},
  pages        = {9},
  volume       = {12},
  abstract     = {This paper presents a review kind of data compression techniques. Data compression is widely used by the community because through a compression we can save storage. Data compression can also speed up a transmission of data from one person to another. In performing a compression requires a method of data compression that can be used, the method can then be used to compress a data. Data that can be compressed not only text data but can be images and video. Data compression technique is divided into 2 namely lossy compression and lossless compression. But which is often used to perform a compression that is lossless compression. A kind of lossless compressions such as Huffman, Shannon Fano, Tunstall, Lempel Ziv welch and run-length encoding. Each method has the ability to perform a different compression. This paper explains how a method works in doing a compression and explains which method is well used in doing a data compression in the form of text. The output generated in doing a can be known through the compression file size that becomes smaller than the original file.},
  file         = {2017 A Review of Data Compression Techniques - Fitriya et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\2017 A Review of Data Compression Techniques - Fitriya et al..pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
}

@Article{2019CalhounExploring,
  author         = {Calhoun, Jon and Cappello, Franck and Olson, Luke N and Snir, Marc and Gropp, William D},
  date           = {2019-03},
  journaltitle   = {The International Journal of High Performance Computing Applications},
  title          = {Exploring the feasibility of lossy compression for {PDE} simulations},
  doi            = {10.1177/1094342018762036},
  issn           = {1094-3420, 1741-2846},
  number         = {2},
  pages          = {397--410},
  url            = {http://journals.sagepub.com/doi/10.1177/1094342018762036},
  urldate        = {2021-03-03},
  volume         = {33},
  abstract       = {Checkpoint restart plays an important role in high-performance computing ({HPC}) applications, allowing simulation runtime to extend beyond a single job allocation and facilitating recovery from hardware failure. Yet, as machines grow in size and in complexity, traditional approaches to checkpoint restart are becoming prohibitive. Current methods store a subset of the application’s state and exploit the memory hierarchy in the machine. However, as the energy cost of data movement continues to dominate, further reductions in checkpoint size are needed. Lossy compression, which can significantly reduce checkpoint sizes, offers a potential to reduce computational cost in checkpoint restart. This article investigates the use of numerical properties of partial differential equation ({PDE}) simulations, such as bounds on the truncation error, to evaluate the feasibility of using lossy compression in checkpointing {PDE} simulations. Restart from a checkpoint with lossy compression is considered for a fail-stop error in two time-dependent {HPC} application codes: {PlasComCM} and Nek5000. Results show that error in application variables due to a restart from a lossy compressed checkpoint can be masked by the numerical error in the discretization, leading to increased efficiency in checkpoint restart without influencing overall accuracy in the simulation.},
  file           = {2019 Exploring the feasibility of lossy compression for PDE simulations - Calhoun et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\2019 Exploring the feasibility of lossy compression for PDE simulations - Calhoun et al..pdf:application/pdf},
  keywords       = {read, prio1, qualityAssured, relevant},
  langid         = {english},
  priority       = {prio1},
  qualityassured = {qualityAssured},
  readstatus     = {read},
  relevance      = {relevant},
  shortjournal   = {The International Journal of High Performance Computing Applications},
}

@Article{2014CanovasLossy,
  author       = {Cánovas, Rodrigo and Moffat, Alistair and Turpin, Andrew},
  date         = {2014-08-01},
  journaltitle = {Bioinformatics},
  title        = {Lossy compression of quality scores in genomic data},
  doi          = {10.1093/bioinformatics/btu183},
  issn         = {1460-2059, 1367-4803},
  number       = {15},
  pages        = {2130--2136},
  url          = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btu183},
  urldate      = {2021-03-03},
  volume       = {30},
  abstract     = {Motivation: Next-generation sequencing technologies are revolutionizing medicine. Data from sequencing technologies are typically represented as a string of bases, an associated sequence of per-base quality scores and other metadata, and in aggregate can require a large amount of space. The quality scores show how accurate the bases are with respect to the sequencing process, that is, how confident the sequencer is of having called them correctly, and are the largest component in datasets in which they are retained. Previous research has examined how to store sequences of bases effectively; here we add to that knowledge by examining methods for compressing quality scores. The quality values originate in a continuous domain, and so if a fidelity criterion is introduced, it is possible to introduce flexibility in the way these values are represented, allowing lossy compression over the quality score data.},
  file         = {2014 Lossy compression of quality scores in genomic data - Cánovas et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\2014 Lossy compression of quality scores in genomic data - Cánovas et al..pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
}

@Article{2019CunninghamSubjective,
  author       = {Cunningham, Stuart and {McGregor}, Iain},
  date         = {2019-07-11},
  journaltitle = {International Journal of Digital Multimedia Broadcasting},
  title        = {Subjective Evaluation of Music Compressed with the {ACER} Codec Compared to {AAC}, {MP}3, and Uncompressed {PCM}},
  doi          = {10.1155/2019/8265301},
  issn         = {1687-7578, 1687-7586},
  pages        = {1--16},
  url          = {https://www.hindawi.com/journals/ijdmb/2019/8265301/},
  urldate      = {2021-03-03},
  volume       = {2019},
  abstract     = {Audio data compression has revolutionised the way in which the music industry and musicians sell and distribute their products. Our previous research presented a novel codec named {ACER} (Audio Compression Exploiting Repetition), which achieves data reduction by exploiting irrelevancy and redundancy in musical structure whilst generally maintaining acceptable levels of noise and distortion in objective evaluations. However, previous work did not evaluate {ACER} using subjective listening tests, leaving a gap to demonstrate its applicability under human audio perception tests. In this paper, we present a double-blind listening test that was conducted with a range of listeners (N=100). The aim was to determine the efficacy of the {ACER} codec, in terms of perceptible noise and spatial distortion artefacts, against de facto standards for audio data compression and an uncompressed reference. Results show that participants reported no perceived differences between the uncompressed, {MP}3, {AAC}, {ACER} high quality, and {ACER} medium quality compressed audio in terms of noise and distortions but that the {ACER} low quality format was perceived as being of lower quality. However, in terms of participants’ perceptions of the stereo field, all formats under test performed as well as each other, with no statistically significant differences. A qualitative, thematic analysis of listeners’ feedback revealed that the noise artefacts that produced the {ACER} technique are different from those of comparator codecs, reflecting its novel approach. Results show that the quality of contemporary audio compression systems has reached a stage where their performance is perceived to be as good as uncompressed audio. The {ACER} format is able to compete as an alternative, with results showing a preference for the {ACER} medium quality versions over {WAV}, {MP}3, and {AAC}. The {ACER} process itself is viable on its own or in conjunction with techniques such as {MP}3 and {AAC}.},
  file         = {2019 Subjective Evaluation of Music Compressed with the ACER Codec Compared to AAC, MP3, and Uncompressed PCM - Cunningham and McGregor.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2019 Subjective Evaluation of Music Compressed with the ACER Codec Compared to AAC, MP3, and Uncompressed PCM - Cunningham and McGregor.pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
  shortjournal = {International Journal of Digital Multimedia Broadcasting},
}

@Article{2019HidayatCritical,
  author       = {Hidayat, Tonny and Zakaria, Mohd Hafiz and Che Pee, Naim},
  date         = {2019-01-18},
  journaltitle = {International journal of simulation: systems, science \& technology},
  title        = {A Critical Assessment of Advanced Coding Standards for Lossless Audio Compression},
  doi          = {10.5013/IJSSST.a.19.05.31},
  issn         = {1473-804X},
  url          = {https://edas.info/doi/10.5013/IJSSST.a.19.05.31},
  urldate      = {2021-03-03},
  abstract     = {Image data, text, video, and audio data all require compression for storage issues and real-time access via computer networks. Audio data cannot use compression technique for generic data. The use of algorithms leads to poor sound quality, small compression ratios and algorithms are not designed for real-time access. Lossless audio compression has achieved observation as a research topic and business field of the importance of the need to store data with excellent condition and larger storage charges. This article will discuss and analyze the various lossless and standardized audio coding algorithms that concern about {LPC} definitely due to its reputation and resistance to compression that is audio. However, another expectation plans are likewise broke down for relative materials. Comprehension of {LPC} improvements, for example, {LSP} deterioration procedures is additionally examined in this paper.},
  file         = {2019 A Critical Assessment of Advanced Coding Standards for Lossless Audio Compression - Hidayat et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2019 A Critical Assessment of Advanced Coding Standards for Lossless Audio Compression - Hidayat et al..pdf:application/pdf},
  langid       = {english},
}

@Article{2019UlachaHigh,
  author       = {{Ulacha} and {Wernik}},
  date         = {2019-11-30},
  journaltitle = {Applied Sciences},
  title        = {A High Efficiency Multistage Coder for Lossless Audio Compression using {OLS}+ and {CDCCR} method},
  doi          = {10.3390/app9235218},
  issn         = {2076-3417},
  number       = {23},
  pages        = {5218},
  url          = {https://www.mdpi.com/2076-3417/9/23/5218},
  urldate      = {2021-03-03},
  volume       = {9},
  abstract     = {In this paper, the improvement of the cascaded prediction method was presented. Three types of main predictor block with diﬀerent levels of complexity were compared, including two complex prediction methods with backward adaptation, i.e., extension Active Level Classiﬁcation Model ({ALCM}+) and extended Ordinary Least Square ({OLS}+). Our own approach to implementation of the eﬀective context-dependent constant component removal block is also presented. Additionally, the improved adaptive arithmetic coder with short, medium and long-term adaptation was presented, and the experiment was carried out comparing the results with other known lossless audio coders against which our method obtained the best eﬃciency.},
  file         = {2019 A High Efficiency Multistage Coder for Lossless Audio Compression using OLS+ and CDCCR method - Ulacha and Wernik.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2019 A High Efficiency Multistage Coder for Lossless Audio Compression using OLS+ and CDCCR method - Ulacha and Wernik.pdf:application/pdf},
  langid       = {english},
  shortjournal = {Applied Sciences},
}

@Article{2020O’GradyRethinking,
  author       = {O’Grady, Pat},
  date         = {2020-12-09},
  journaltitle = {Convergence: The International Journal of Research into New Media Technologies},
  title        = {Rethinking criticism about lossy compression: Sound fidelity, large-scale production and audio capital in pop music},
  doi          = {10.1177/1354856520976454},
  issn         = {1354-8565, 1748-7382},
  pages        = {135485652097645},
  url          = {http://journals.sagepub.com/doi/10.1177/1354856520976454},
  urldate      = {2021-03-03},
  abstract     = {Although digital music streaming and downloading practices highlight a preference for portability over fidelity among pop music consumers, for some ‘high fidelity’ continues to be a crucial component of their pop music engagement. This article examines the political dimensions of audio preferences. It considers comments by Neil Young and those within The Distortion of Sound who claim that the lossy compression encoding process – used in digital downloading and streaming –significantly degrades the quality of recorded sound and compromises valuable pop music characteristics. In 2015, these claims were followed by the development of the ‘hi-res’ music player device and digital download store ‘Pono’. This article argues that some criticism of lossy compression can be understood as ‘audio capital’. Rather than merely exchanging aesthetic preferences, or a reliable metric of the audibility of high fidelity, audio capital describes the reconversion of celebrity status and economic capital into a class divide structured by an appreciation for large-scale music production.},
  file         = {2020 Rethinking criticism about lossy compression Sound fidelity, large-scale production and audio capital in pop music - O’Grady.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2020 Rethinking criticism about lossy compression Sound fidelity, large-scale production and audio capital in pop music - O’Grady.pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
  shortjournal = {Convergence},
  shorttitle   = {Rethinking criticism about lossy compression},
}

@InProceedings{2019BlauRethinking,
  author       = {Blau, Yochai and Michaeli, Tomer},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
  date         = {2019},
  title        = {Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff},
  pages        = {11},
  abstract     = {Lossy compression algorithms are typically designed and analyzed through the lens of Shannon’s rate-distortion theory, where the goal is to achieve the lowest possible distortion (e.g., low {MSE} or high {SSIM}) at any given bit rate. However, in recent years, it has become increasingly accepted that “low distortion” is not a synonym for “high perceptual quality”, and in fact optimization of one often comes at the expense of the other. In light of this understanding, it is natural to seek for a generalization of rate-distortion theory which takes perceptual quality into account. In this paper, we adopt the mathematical deﬁnition of perceptual quality recently proposed by Blau \& Michaeli (2018), and use it to study the three-way tradeoff between rate, distortion, and perception. We show that restricting the perceptual quality to be high, generally leads to an elevation of the rate-distortion curve, thus necessitating a sacriﬁce in either rate or distortion. We prove several fundamental properties of this triple-tradeoff, calculate it in closed form for a Bernoulli source, and illustrate it visually on a toy {MNIST} example.},
  file         = {Rethinking Lossy Compression The Rate-Distortion-Perception Tradeoff - Blau and Michaeli.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\Rethinking Lossy Compression The Rate-Distortion-Perception Tradeoff - Blau and Michaeli.pdf:application/pdf},
  journaltitle = {Proceedings of the 36th International Conference on Machine Learning},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
}

@InProceedings{1999BrandenburgMP3,
  author       = {Brandenburg, Karlheinz},
  booktitle    = {AES 17thInternational Conference on High Quality Audio Coding},
  date         = {1999},
  title        = {{MP}3 and {AAC} Explained},
  pages        = {12},
  file         = {MP3 and AAC Explained - Brandenburg.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\MP3 and AAC Explained - Brandenburg.pdf:application/pdf},
  journaltitle = {AES 17thInternational Conference on High Quality Audio Coding},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
}

@InProceedings{2016SiegertMeasuring,
  author    = {Ingo Siegert and Alicia Flores Lotz and Linh Linda Duong and Andreas Wendemuth},
  booktitle = {Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung 2016},
  title     = {Measuring the Impact of Audio Compression on the Spectral Quality of Speech Data},
  editor    = {Oliver Jokisch},
  isbn      = {978-3-959080-40-8},
  pages     = {229--236},
  publisher = {TUDpress, Dresden},
  url       = {http://www.essv.de/paper.php?id=344},
  file      = {MEASURING THE IMPACT OF AUDIO COMPRESSION ON THE SPECTRAL QUALITY OF SPEECH DATA - Siegert et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\MEASURING THE IMPACT OF AUDIO COMPRESSION ON THE SPECTRAL QUALITY OF SPEECH DATA - Siegert et al..pdf:application/pdf},
  keywords  = {Audio- und Sprachkodierung, Qualitätsbewertung},
  year      = {2016},
}

@Article{2021HuAudio,
  author       = {Hu, Chenhao and Wang, Xiaochen and Hu, Ruimin and Wu, Yulin},
  date         = {2021-02-19},
  journaltitle = {Multimedia Tools and Applications},
  title        = {Audio object coding based on N-step residual compensating},
  doi          = {10.1007/s11042-020-10339-0},
  issn         = {1380-7501, 1573-7721},
  url          = {http://link.springer.com/10.1007/s11042-020-10339-0},
  urldate      = {2021-03-03},
  abstract     = {Object-based audio techniques provide more flexibility and convenience for personalized rendering under various playback configurations. Many methods have been proposed to encode and transmit multiple audio objects at a low bit-rate. However, the recovered audio objects have severe frequency aliasing distortion, which will destroy the immersive sound quality. This paper describes a new structure to reduce every object’s aliasing distortion. In this method, we extract residual and gain parameters of all objects after N-step operation and use singular value decomposition to compress the residual matrices. The residual matrices can compensate for aliasing distortion in the decoding part. Moreover, we find a proper ordering strategy experimentally to determine the object coding order because it will affect the final decoded quality. From experiment results, the energy sorting strategy is chosen as the best ordering strategy, and the residual information bit-rate can be reduced from 14.11 kbps/per object to 5.87 kbps/per object. Compared with previous studies, our method gets better performance in objective and subjective experiments. The proposed N-step residual compensating structure can reduce every object’s aliasing distortion better than the state-ofthe-art methods.},
  file         = {2021 Audio object coding based on N-step residual compensating - Hu et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2021 Audio object coding based on N-step residual compensating - Hu et al..pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
  shortjournal = {Multimed Tools Appl},
}

@InProceedings{2018KwanObjective,
  author     = {Kwan, Chiman and Luk, Yvonne},
  booktitle  = {2018 Data Compression Conference},
  date       = {2018-03},
  title      = {Objective Performance Evaluation of Several State-of-the-Art Audio Codecs},
  doi        = {10.1109/DCC.2018.00071},
  eventtitle = {2018 Data Compression Conference ({DCC})},
  isbn       = {978-1-5386-4883-4},
  location   = {Snowbird, {UT}},
  pages      = {418--418},
  publisher  = {{IEEE}},
  url        = {https://ieeexplore.ieee.org/document/8416635/},
  urldate    = {2021-03-03},
  file       = {2018 Objective Performance Evaluation of Several State-of-the-Art Audio Codecs - Kwan and Luk.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2018 Objective Performance Evaluation of Several State-of-the-Art Audio Codecs - Kwan and Luk.pdf:application/pdf},
  keywords   = {relevant},
  relevance  = {relevant},
}

@InProceedings{2019ZhurakovskyiComparative,
  author       = {Zhurakovskyi, Bohdan and Tsopa, Nataliia and Batrak, Yevhenii and Odarchenko, Roman and Smirnova, Tetiana},
  booktitle    = {Proceedings of the International Workshop on Cyber Hygiene},
  date         = {2019},
  title        = {Comparative Analysis of Modern Formats of Lossy Audio Compression},
  pages        = {13},
  abstract     = {In the modern world, computer engineering is developing high-speed – frequency and performance of processes are growing, storage space is increasing, and storage access time is speeding up. However, while the speed of different devices is extremely growing, the speed of data transfer is increasing significantly slower. The peculiarity of the majority types of data is their redundancy. Due to this, nowadays the compression algorithms are widely used to provide efficient storage and transfer of information. The objective of the work is the study and analysis of modern formats of lossy audio information compression. At present, a significant number of formats of audio compression are used. They all have pros and cons. This study is performed to find out which format is the most effective and successfully replaced {MP}3 – one of the most popular formats.},
  file         = {2020 Comparative Analysis of Modern Formats of Lossy Audio Compression - Zhurakovskyi et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2020 Comparative Analysis of Modern Formats of Lossy Audio Compression - Zhurakovskyi et al..pdf:application/pdf},
  journaltitle = {Proceedings of the International Workshop on Cyber Hygiene},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
}

@Article{2014ZordanPerformance,
  author       = {Zordan, Davide and Martinez, Borja and Vilajosana, Ignasi and Rossi, Michele},
  date         = {2014-11-07},
  journaltitle = {{ACM} Transactions on Sensor Networks},
  title        = {On the Performance of Lossy Compression Schemes for Energy Constrained Sensor Networking},
  doi          = {10.1145/2629660},
  issn         = {1550-4859, 1550-4867},
  number       = {1},
  pages        = {1--34},
  url          = {https://dl.acm.org/doi/10.1145/2629660},
  urldate      = {2021-03-03},
  volume       = {11},
  abstract     = {Lossy temporal compression is key for energy-constrained wireless sensor networks ({WSNs}), where the imperfect reconstruction of the signal is often acceptable at the data collector, subject to some maximum error tolerance. In this article, we evaluate a number of selected lossy compression methods from the literature and extensively analyze their performance in terms of compression efficiency, computational complexity, and energy consumption. Specifically, we first carry out a performance evaluation of existing and new compression schemes, considering linear, autoregressive, {FFT}-/{DCT}- and wavelet-based models , by looking at their performance as a function of relevant signal statistics. Second, we obtain formulas through numerical fittings to gauge their overall energy consumption and signal representation accuracy. Third, we evaluate the benefits that lossy compression methods bring about in interference-limited multihop networks, where the channel access is a source of inefficiency due to collisions and transmission scheduling. Our results reveal that the {DCT}-based schemes are the best option in terms of compression efficiency but are inefficient in terms of energy consumption. Instead, linear methods lead to substantial savings in terms of energy expenditure by, at the same time, leading to satisfactory compression ratios, reduced network delay, and increased reliability performance.},
  file         = {2014 On the Performance of Lossy Compression Schemes for Energy Constrained Sensor Networking - Zordan et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2014 On the Performance of Lossy Compression Schemes for Energy Constrained Sensor Networking - Zordan et al..pdf:application/pdf},
  langid       = {english},
  shortjournal = {{ACM} Trans. Sen. Netw.},
}

@Article{1993SinhaLow,
  author       = {Sinha, D. and Tewfik, A.H.},
  date         = {1993-12},
  journaltitle = {{IEEE} Transactions on Signal Processing},
  title        = {Low bit rate transparent audio compression using adapted wavelets},
  doi          = {10.1109/78.258086},
  issn         = {1053587X},
  number       = {12},
  pages        = {3463--3479},
  url          = {http://ieeexplore.ieee.org/document/258086/},
  urldate      = {2021-03-03},
  volume       = {41},
  file         = {1993 Low bit rate transparent audio compression using adapted wavelets - Sinha and Tewfik.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\1993 Low bit rate transparent audio compression using adapted wavelets - Sinha and Tewfik.pdf:application/pdf},
  langid       = {english},
  shortjournal = {{IEEE} Trans. Signal Process.},
}

@Article{1998LevineSinesTransientsNoise,
  author       = {Scott N. Levine and Julius O. Smith III},
  date         = {1998},
  journaltitle = {Audio Engineering Society Convention},
  title        = {A Sines+Transients+Noise Audio Representation for Data Compression and Time Pitch Scale Modi cations},
  pages        = {21},
  abstract     = {The purpose of this paper is to demonstrate a low bitrate audio coding algorithm that allows modi cations in the compressed domain. The input audio is segregated into three di erent representations: sinusoids, transients, and noise. Each representation can be individually quantized, and then easily be time-scaled and or pitch-shifted.},
  file         = {A Sines+Transients+Noise Audio Representation for Data Compression and Time Pitch Scale Modi cations - Levine.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\A Sines+Transients+Noise Audio Representation for Data Compression and Time Pitch Scale Modi cations - Levine.pdf:application/pdf},
  langid       = {english},
}

@Article{1998SrinivasanHigh,
  author       = {Srinivasan, P. and Jamieson, L.H.},
  date         = {1998-04},
  journaltitle = {{IEEE} Transactions on Signal Processing},
  title        = {High-quality audio compression using an adaptive wavelet packet decomposition and psychoacoustic modeling},
  doi          = {10.1109/78.668558},
  issn         = {1053587X},
  number       = {4},
  pages        = {1085--1093},
  url          = {http://ieeexplore.ieee.org/document/668558/},
  urldate      = {2021-03-03},
  volume       = {46},
  abstract     = {This paper presents a technique to incorporate psychoacoustic models into an adaptive wavelet packet scheme to achieve perceptually transparent compression of high-quality (44.1 {kHz}) audio signals at about 45 kb/s. The ﬁlter bank structure adapts according to psychoacoustic criteria and according to the computational complexity that is available at the decoder. This permits software implementations that can perform according to the computational power available in order to achieve real time coding/decoding. The bit allocation scheme is an adapted zero-tree algorithm that also takes input from the psychoacoustic model. The measure of performance is a quantity called subband perceptual rate, which the ﬁlter bank structure adapts to approach the perceptual entropy ({PE}) as closely as possible. In addition, this method is also amenable to progressive transmission, that is, it can achieve the best quality of reconstruction possible considering the size of the bit stream available at the encoder. The result is a variablerate compression scheme for high-quality audio that takes into account the allowed computational complexity, the available bitbudget, and the psychoacoustic criteria for transparent coding. This paper thus provides a novel scheme to marry the results in wavelet packets and perceptual coding to construct an algorithm that is well suited to high-quality audio transfer for internet and storage applications.},
  file         = {1998 High-quality audio compression using an adaptive wavelet packet decomposition and psychoacoustic modeling - Srinivasan and Jamieson.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\1998 High-quality audio compression using an adaptive wavelet packet decomposition and psychoacoustic modeling - Srinivasan and Jamieson.pdf:application/pdf},
  langid       = {english},
  shortjournal = {{IEEE} Trans. Signal Process.},
}

@Article{2001HansLossless,
  author       = {Hans, M. and Schafer, R.W.},
  date         = {2001-07},
  journaltitle = {{IEEE} Signal Processing Magazine},
  title        = {Lossless compression of digital audio},
  doi          = {10.1109/79.939834},
  issn         = {10535888},
  number       = {4},
  pages        = {21--32},
  url          = {http://ieeexplore.ieee.org/document/939834/},
  urldate      = {2021-03-03},
  volume       = {18},
  file         = {2001 Lossless compression of digital audio - Hans and Schafer.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2001 Lossless compression of digital audio - Hans and Schafer.pdf:application/pdf},
  keywords     = {relevant},
  relevance    = {relevant},
  shortjournal = {{IEEE} Signal Process. Mag.},
}

@Article{2008GoyalCompressive,
  author       = {Goyal, V.K. and Fletcher, A.K. and Rangan, S.},
  date         = {2008-03},
  journaltitle = {{IEEE} Signal Processing Magazine},
  title        = {Compressive Sampling and Lossy Compression},
  doi          = {10.1109/MSP.2007.915001},
  issn         = {1053-5888},
  number       = {2},
  pages        = {48--56},
  url          = {http://ieeexplore.ieee.org/document/4472243/},
  urldate      = {2021-03-03},
  volume       = {25},
  file         = {2008 Compressive Sampling and Lossy Compression - Goyal et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2008 Compressive Sampling and Lossy Compression - Goyal et al..pdf:application/pdf},
  langid       = {english},
  shortjournal = {{IEEE} Signal Process. Mag.},
}

@Article{2013PatilAudio,
  author       = {Patil, M V and Gupta, Apoorva and Varma, Ankita and Salil, Shikhar},
  date         = {2013},
  journaltitle = {International Journal of Innovative Research in Science, Engineering and Technology},
  title        = {Audio and Speech Compression Using {DCT} and {DWT} Techniques},
  issn         = {2319-8753},
  number       = {5},
  pages        = {9},
  volume       = {2},
  abstract     = {Audio compression has become one of the basic technologies of the multimedia age. The change in the telecommunication infrastructure, in recent years, from circuit switched to packet switched systems has also reflected on the way that speech and audio signals are carried in present systems. In many applications, such as the design of multimedia workstations and high quality audio transmission and storage, the goal is to achieve transparent coding of audio and speech signals at the lowest possible data rates. In other words, bandwidth cost money, therefore, the transmission and storage of information becomes costly. However, if we can use less data, both transmission and storage become cheaper. Further reduction in bit rate is an attractive proposition in applications like remote broadcast lines, studio links, satellite transmission of high quality audio and voice over internet.},
  file         = {2013 Audio and Speech Compression Using DCT and DWT Techniques - Patil et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2013 Audio and Speech Compression Using DCT and DWT Techniques - Patil et al..pdf:application/pdf},
  langid       = {english},
}

@Article{2013AroraAudio,
  author       = {Arora, Manas and Maurya, Neha},
  date         = {2013},
  journaltitle = {International Journal of Scientific and Research Publications},
  title        = {Audio Compression in {MPEG} Technology},
  issn         = {2250-3153},
  number       = {12},
  pages        = {4},
  volume       = {3},
  abstract     = {{MPEG} stands for {MOVING} {PICTURE} {EXPERTS} {GROUP} is a standard for video and audio compression for eliminating the noisy signals from the transmitted signals from the satellite. Audio compression is a basic method defined under {MPEG}-1 and {MPEG}-4 which by coding techniques compress audio signals to filter out undesired signals .This paper focuses on the {MPEG} technology, need and coding technique for audio compression.},
  file         = {2013 Audio Compression in MPEG Technology - Arora and Maurya.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2013 Audio Compression in MPEG Technology - Arora and Maurya.pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
}

@InProceedings{2016SeichterAAC,
  author     = {Seichter, Daniel and Cuccovillo, Luca and Aichroth, Patrick},
  booktitle  = {2016 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  date       = {2016-03},
  title      = {{AAC} encoding detection and bitrate estimation using a convolutional neural network},
  doi        = {10.1109/ICASSP.2016.7472041},
  eventtitle = {2016 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  isbn       = {978-1-4799-9988-0},
  location   = {Shanghai},
  pages      = {2069--2073},
  publisher  = {{IEEE}},
  url        = {http://ieeexplore.ieee.org/document/7472041/},
  urldate    = {2021-03-03},
  abstract   = {In this paper, we propose a new method for {AAC} encoding detection and bitrate estimation from {PCM} material. The algorithm is based on a Convolutional Neural Network that can distinguish between eight different bitrates. It achieves an average accuracy of 94.65\% by analysis of only 116.10 ms of content.},
  file       = {2016 AAC encoding detection and bitrate estimation using a convolutional neural network - Seichter et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2016 AAC encoding detection and bitrate estimation using a convolutional neural network - Seichter et al..pdf:application/pdf},
  langid     = {english},
}

@InProceedings{2017HennequinCodec,
  author     = {Hennequin, Romain and Royo-Letelier, Jimena and Moussallam, Manuel},
  booktitle  = {2017 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  date       = {2017-03},
  title      = {Codec independent lossy audio compression detection},
  doi        = {10.1109/ICASSP.2017.7952251},
  eventtitle = {2017 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  isbn       = {978-1-5090-4117-6},
  location   = {New Orleans, {LA}},
  pages      = {726--730},
  publisher  = {{IEEE}},
  url        = {http://ieeexplore.ieee.org/document/7952251/},
  urldate    = {2021-03-03},
  abstract   = {In this paper, we propose a method for detecting marks of lossy compression encoding, such as {MP}3 or {AAC}, from {PCM} audio. The method is based on a convolutional neural network ({CNN}) applied to audio spectrograms and trained with the output of various lossy audio codecs and bitrates. Our method shows good performances on a large database and robustness to codec type and resampling.},
  file       = {2017 Codec independent lossy audio compression detection - Hennequin et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2017 Codec independent lossy audio compression detection - Hennequin et al..pdf:application/pdf},
  langid     = {english},
}

@InProceedings{2018HuangBandwidth,
  author    = {Huang, Qingbo and Wu, Xihong and Qu, Tianshu},
  booktitle = {AES 144th Convention, Milan, Italy, 2018 May 23–26},
  date      = {2018},
  title     = {Bandwidth extension method based on generative adversarial nets for audio compression},
  pages     = {8},
  abstract  = {The compression ratio of core-encoder can be improved significantly by reducing the bandwidth of the audio signal, resulting in the poor listening perception. This paper proposes a bandwidth extension method based on generative adversarial nets ({GAN}) for extending the bandwidth of an audio signal, to create a more natural sound. The method uses {GAN} as a generative model to fit the distribution of the {MDCT} coefficients of the audio signals in the high-frequency components. Through minimax two-player gaming, more natural high-frequency information can be estimated. On this basis, a codec system is built up. To evaluate the proposed bandwidth extension system the {MUSHRA} experiments were carried on and the results show that there is comparable performance with {HE}-{AAC}.},
  file      = {2018 Bandwidth extension method based on generative adversarial nets for audio compression - Huang et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2018 Bandwidth extension method based on generative adversarial nets for audio compression - Huang et al..pdf:application/pdf},
  keywords  = {relevant},
  langid    = {english},
  relevance = {relevant},
}

@InProceedings{2000EdlerUniversity,
  author    = {Edler, Bernd and Purnhagen, Heiko},
  booktitle = {{WCC} 2000 - {ICCT} 2000. 2000 International Conference on Communication Technology Proceedings (Cat. No.00EX420)},
  date      = {2000},
  title     = {Parametric audio coding},
  doi       = {10.1109/ICCT.2000.889276},
  pages     = {4},
  publisher = {{IEEE}},
  abstract  = {For very low bit rate audio coding applications in mobile communications or on the internet, parametric audio coding has evolved as a technique complementing the more traditional approaches. These are transform codecs originally designed for achieving {CDlike} quality on one hand, and specialized speech codecs on the other hand. Both of these techniques usually represent the audio signal waveform in a way such that the decoder output signal gives an approximation of the encoder input signal, while taking into account perceptual criteria. Compared to this approach, in parametric audio coding the models of the signal source and of human perception are extended. The source model is now based on the assumption that the audio signal is the sum of “components,” each of which can be approximated by a relatively simple signal model with a small number of parameters. The perception model is based on the assumption that the sound of the decoder output signal should be as similar as possible to that of the encoder input signal. Therefore, the approximation of waveforms is no longer necessary. This approach can lead to a very efﬁcient representation. However, a suitable set of models for signal components, a good decomposition, and a good parameter estimation are all vital for achieving maximum audio quality.},
  file      = {University of Hannover Laboratorium fu¨r Informationstechnologie Schneiderberg 32, 30167 Hannover, Germany - Edler and Purnhagen.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\University of Hannover Laboratorium fu¨r Informationstechnologie Schneiderberg 32, 30167 Hannover, Germany - Edler and Purnhagen.pdf:application/pdf},
  langid    = {english},
}

@Article{2014CunninghamData,
  author       = {Cunningham, Stuart and Grout, Vic},
  date         = {2014-10},
  journaltitle = {Multimedia Tools and Applications},
  title        = {Data reduction of audio by exploiting musical repetition},
  doi          = {10.1007/s11042-013-1504-y},
  issn         = {1380-7501, 1573-7721},
  number       = {3},
  pages        = {2299--2320},
  url          = {http://link.springer.com/10.1007/s11042-013-1504-y},
  urldate      = {2021-03-03},
  volume       = {72},
  abstract     = {This paper presents and evaluates a method of audio compression specifically designed to exploit the natural repetition that occurs within musical audio. Our system is entitled Audio Compression Exploiting Repetition ({ACER}). {ACER} is a perceptual technique, but one that does not consider exploiting masking, but rather attempts to apply the principles of Lempel-Ziv and run-length encoding, by substituting audio sequences for numeric or character strings. The {ACER} procedure applies a pseudo exhaustive search process and spectral difference grading. Since {ACER} exploits musical structure, the amount of data reduction achieved varies from pieceto-piece. The system is described before results on a corpus of material are presented. The analysis shows moderate amounts of data reduction take place whilst the system is operating within parameters designed to maintain high-levels of perceptual audio quality, whilst lower rates of perceptual quality yield greater data reduction. Objective quality evaluations are conducted that reveal degradation in fidelity that is relative to the compression parameters.},
  file         = {2014 Data reduction of audio by exploiting musical repetition - Cunningham and Grout.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2014 Data reduction of audio by exploiting musical repetition - Cunningham and Grout.pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
  shortjournal = {Multimed Tools Appl},
}

@Article{2016MoorePreferred,
  author       = {Moore, Brian C. J. and Sęk, Aleksander},
  date         = {2016-01-01},
  journaltitle = {Trends in Hearing},
  title        = {Preferred Compression Speed for Speech and Music and Its Relationship to Sensitivity to Temporal Fine Structure},
  doi          = {10.1177/2331216516640486},
  issn         = {2331-2165, 2331-2165},
  pages        = {233121651664048},
  url          = {http://journals.sagepub.com/doi/10.1177/2331216516640486},
  urldate      = {2021-03-03},
  volume       = {20},
  abstract     = {Multichannel amplitude compression is widely used in hearing aids. The preferred compression speed varies across individuals. Moore (2008) suggested that reduced sensitivity to temporal fine structure ({TFS}) may be associated with preference for slow compression. This idea was tested using a simulated hearing aid. It was also assessed whether preferences for compression speed depend on the type of stimulus: speech or music. Twenty-two hearing-impaired subjects were tested, and the stimulated hearing aid was fitted individually using the {CAM}2A method. On each trial, a given segment of speech or music was presented twice. One segment was processed with fast compression and the other with slow compression, and the order was balanced across trials. The subject indicated which segment was preferred and by how much. On average, slow compression was preferred over fast compression, more so for music, but there were distinct individual differences, which were highly correlated for speech and music. Sensitivity to {TFS} was assessed using the difference limen for frequency at 2000 Hz and by two measures of sensitivity to interaural phase at low frequencies. The results for the difference limens for frequency, but not the measures of sensitivity to interaural phase, supported the suggestion that preference for compression speed is affected by sensitivity to {TFS}.},
  file         = {2016 Preferred Compression Speed for Speech and Music and Its Relationship to Sensitivity to Temporal Fine Structure - Moore and Sęk.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2016 Preferred Compression Speed for Speech and Music and Its Relationship to Sensitivity to Temporal Fine Structure - Moore and Sęk.pdf:application/pdf},
  langid       = {english},
  shortjournal = {Trends in Hearing},
}

@Article{2017Marzenevolution,
  author       = {Marzen, Sarah E. and {DeDeo}, Simon},
  date         = {2017-05},
  journaltitle = {Journal of The Royal Society Interface},
  title        = {The evolution of lossy compression},
  doi          = {10.1098/rsif.2017.0166},
  issn         = {1742-5689, 1742-5662},
  number       = {130},
  pages        = {20170166},
  url          = {https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0166},
  urldate      = {2021-03-03},
  volume       = {14},
  abstract     = {In complex environments, there are costs to both ignorance and perception. An organism needs to track fitness-relevant information about its world, but the more information it tracks, the more resources it must devote to perception. As a first step towards a general understanding of this trade-off, we use a tool from information theory, rate–distortion theory, to study large, unstructured environments with fixed, randomly drawn penalties for stimuli confusion (‘distortions’). We identify two distinct regimes for organisms in these environments: a high-fidelity regime where perceptual costs grow linearly with environmental complexity, and a low-fidelity regime where perceptual costs are, remarkably, independent of the number of environmental states. This suggests that in environments of rapidly increasing complexity, well-adapted organisms will find themselves able to make, just barely, the most subtle distinctions in their environment.},
  file         = {2017 The evolution of lossy compression - Marzen and DeDeo.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2017 The evolution of lossy compression - Marzen and DeDeo.pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
  shortjournal = {J. R. Soc. Interface.},
}

@InProceedings{2017Muinreview,
  author     = {Muin, Fathiah Abdul and Gunawan, Teddy Surya and Kartiwi, Mira and Elsheikh, Elsheikh M. A.},
  booktitle  = {AIP Conference Proceedings},
  date       = {2017},
  title      = {A review of lossless audio compression standards and algorithms},
  doi        = {10.1063/1.5002024},
  eventtitle = {{ADVANCES} {IN} {ELECTRICAL} {AND} {ELECTRONIC} {ENGINEERING}: {FROM} {THEORY} {TO} {APPLICATIONS}: Proceedings of the International Conference on Electrical and Electronic Engineering ({IC}3E 2017)},
  location   = {Johor, Malaysia},
  pages      = {020006},
  publisher  = {Author(s)},
  url        = {http://aip.scitation.org/doi/abs/10.1063/1.5002024},
  urldate    = {2021-03-03},
  abstract   = {Over the years, lossless audio compression has gained popularity as researchers and businesses has become more aware of the need for better quality and higher storage demand. This paper will analyse various lossless audio coding algorithm and standards that are used and available in the market focusing on Linear Predictive Coding ({LPC}) specifically due to its popularity and robustness in audio compression, nevertheless other prediction methods are compared to verify this. Advanced representation of {LPC} such as {LSP} decomposition techniques are also discussed within this paper.},
  file       = {2017 A review of lossless audio compression standards and algorithms - Muin et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2017 A review of lossless audio compression standards and algorithms - Muin et al..pdf:application/pdf},
  keywords   = {relevant},
  langid     = {english},
  relevance  = {relevant},
  year       = {2017},
}

@Article{2018AhmedOptimizing,
  author       = {Ahmed, Rafid and Islam, Md. Sazzadul and Uddin, Jia},
  date         = {2018-02-01},
  journaltitle = {International Journal of Electrical and Computer Engineering ({IJECE})},
  title        = {Optimizing Apple Lossless Audio Codec Algorithm using {NVIDIA} {CUDA} Architecture},
  doi          = {10.11591/ijece.v8i1.pp70-75},
  issn         = {2088-8708, 2088-8708},
  number       = {1},
  pages        = {70},
  url          = {http://ijece.iaescore.com/index.php/IJECE/article/view/10100},
  urldate      = {2021-03-03},
  volume       = {8},
  abstract     = {As majority of the compression algorithms are implementations for {CPU} architecture, the primary focus of our work was to exploit the opportunities of {GPU} parallelism in audio compression. This paper presents an implementation of Apple Lossless Audio Codec ({ALAC}) algorithm by using {NVIDIA} {GPUs} Compute Unified Device Architecture ({CUDA}) Framework. The core idea was to identify the areas where data parallelism could be applied and parallel programming model {CUDA} could be used to execute the identified parallel components on Single Instruction Multiple Thread ({SIMT}) model of {CUDA}. The dataset was retrieved from European Broadcasting Union, Sound Quality Assessment Material ({SQAM}). Faster execution of the algorithm led to execution time reduction when applied to audio coding for large audios. This paper also presents the reduction of power usage due to running the parallel components on {GPU}. Experimental results reveal that we achieve about 80-90\% speedup through {CUDA} on the identified components over its {CPU} implementation while saving {CPU} power consumption.},
  file         = {2018 Optimizing Apple Lossless Audio Codec Algorithm using NVIDIA CUDA Architecture - Ahmed et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2018 Optimizing Apple Lossless Audio Codec Algorithm using NVIDIA CUDA Architecture - Ahmed et al..pdf:application/pdf},
  langid       = {english},
  shortjournal = {{IJECE}},
}

@Article{2018GunawanPerformance,
  author       = {Gunawan, Teddy Surya and Kartiwi, Mira},
  date         = {2018-04-01},
  journaltitle = {Indonesian Journal of Electrical Engineering and Computer Science},
  title        = {Performance Evaluation of Multichannel Audio Compression},
  doi          = {10.11591/ijeecs.v10.i1.pp146-153},
  issn         = {2502-4760, 2502-4752},
  number       = {1},
  pages        = {146},
  url          = {http://ijeecs.iaescore.com/index.php/IJEECS/article/view/10871},
  urldate      = {2021-03-03},
  volume       = {10},
  abstract     = {In recent years, multichannel audio systems are widely used in modern sound devices as it can provide more realistic and engaging experience to the listener. This paper focuses on the performance evaluation of three lossy, i.e. {AAC}, Ogg Vorbis, and Opus, and three lossless compression, i.e. {FLAC}, {TrueAudio}, and {WavPack}, for multichannel audio signals, including stereo, 5.1 and 7.1 channels. Experiments were conducted on the same three audio files but with different channel configurations. The performance of each encoder was evaluated based on its encoding time (averaged over 100 times), data reduction, and audio quality. Usually, there is always a trade-off between the three metrics. To simplify the evaluation, a new integrated performance metric was proposed that combines all the three performance metrics. Using the new measure, {FLAC} was found to be the best lossless compression, while Ogg Vorbis and Opus were found to be the best for lossy compression depends on the channel configuration. This result could be used in determining the proper audio format for multichannel audio systems.},
  file         = {2018 Performance Evaluation of Multichannel Audio Compression - Gunawan and Kartiwi.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2018 Performance Evaluation of Multichannel Audio Compression - Gunawan and Kartiwi.pdf:application/pdf},
  langid       = {english},
  shortjournal = {{IJEECS}},
}

@InProceedings{2018KimLossy,
  author     = {Kim, Bongjun and Rafii, Zafar},
  booktitle  = {2018 26th European Signal Processing Conference ({EUSIPCO})},
  date       = {2018-09},
  title      = {Lossy Audio Compression Identification},
  doi        = {10.23919/EUSIPCO.2018.8553611},
  eventtitle = {2018 26th European Signal Processing Conference ({EUSIPCO})},
  isbn       = {978-90-827970-1-5},
  location   = {Rome},
  pages      = {2459--2463},
  publisher  = {{IEEE}},
  url        = {https://ieeexplore.ieee.org/document/8553611/},
  urldate    = {2021-03-03},
  abstract   = {We propose a system which can estimate from an audio recording that has previously undergone lossy compression the parameters used for the encoding, and therefore identify the corresponding lossy coding format. The system analyzes the audio signal and searches for the compression parameters and framing conditions which match those used for the encoding. In particular, we propose a new metric for measuring traces of compression which is robust to variations in the audio content and a new method for combining the estimates from multiple audio blocks which can reﬁne the results. We evaluated this system with audio excerpts from songs and movies, compressed into various coding formats, using different bit rates, and captured digitally as well as through analog transfer. Results showed that our system can identify the correct format in almost all cases, even at high bit rates and with distorted audio, with an overall accuracy of 0.96.},
  file       = {2018 Lossy Audio Compression Identification - Kim and Rafii.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2018 Lossy Audio Compression Identification - Kim and Rafii.pdf:application/pdf},
  langid     = {english},
}

@InProceedings{2018WernikApplication,
  author     = {Wernik, Cezary and Ulacha, Grzegorz},
  booktitle  = {2018 Signal Processing: Algorithms, Architectures, Arrangements, and Applications ({SPA})},
  date       = {2018-09},
  title      = {Application of adaptive Golomb codes for lossless audio compression},
  doi        = {10.23919/SPA.2018.8563385},
  eventtitle = {2018 Signal Processing: Algorithms, Architectures, Arrangements, and Applications ({SPA})},
  isbn       = {978-83-62065-31-8 978-83-62065-33-2},
  location   = {Poznan},
  pages      = {203--207},
  publisher  = {{IEEE}},
  url        = {https://ieeexplore.ieee.org/document/8563385/},
  urldate    = {2021-03-03},
  abstract   = {In this paper the advantages of the Golomb codes family on example of audio signals coding are presented. Such as low computational complexity, high efficiency and flexibility to adapt to local changes in the probability distribution characteristics of coded data. The effectiveness of the Golomb coder with forward adaptation with three versions of the reverse adaptation coder has been compared. Also, attempts have been made to solve the problem of incomplete adaptation to the distribution encoded audio data to a one-sided geometric distribution, for which the Golomb code is the optimal code.},
  file       = {2018 Application of adaptive Golomb codes for lossless audio compression - Wernik and Ulacha.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2018 Application of adaptive Golomb codes for lossless audio compression - Wernik and Ulacha.pdf:application/pdf},
  langid     = {english},
}

@Article{2019AliImage,
  author       = {Ali, Ahmed Huaasin and Abbas, Ali Nihad and George, Loay Edwar and Mokhtar, Mohd Rosmadi},
  date         = {2019-09-01},
  journaltitle = {Indonesian Journal of Electrical Engineering and Computer Science},
  title        = {Image and audio fractal compression: comprehensive review, enhancements and research directions},
  doi          = {10.11591/ijeecs.v15.i3.pp1564-1570},
  issn         = {2502-4760, 2502-4752},
  number       = {3},
  pages        = {1564},
  url          = {http://ijeecs.iaescore.com/index.php/IJEECS/article/view/16323},
  urldate      = {2021-03-03},
  volume       = {15},
  abstract     = {This study aims to review the recent techniques in digital multimedia compression with respect to fractal coding techniques. It covers the proposed fractal coding methods in audio and image domain and the techniques that were used for accelerating the encoding time process which is considered the main challenge in fractal compression. This study also presents the trends of the researcher's interests in applying fractal coding in image domain in comparison to the audio domain. This review opens directions for further researches in the area of fractal coding in audio compression and removes the obstacles that face its implementation in order to compare fractal audio with the renowned audio compression techniques.},
  file         = {2019 Image and audio fractal compression comprehensive review, enhancements and research directions - Ali et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2019 Image and audio fractal compression comprehensive review, enhancements and research directions - Ali et al..pdf:application/pdf},
  langid       = {english},
  shortjournal = {{IJEECS}},
  shorttitle   = {Image and audio fractal compression},
}

@InProceedings{2018SanturkarGenerative,
  author    = {Santurkar, Shibani and Budden, David and Shavit, Nir},
  booktitle = {2018 Picture Coding Symposium ({PCS})},
  date      = {2017-06-04},
  title     = {Generative Compression},
  doi       = {10.1109/PCS.2018.8456298},
  eprint    = {1703.01467},
  publisher = {{IEEE}},
  url       = {http://arxiv.org/abs/1703.01467},
  urldate   = {2021-03-03},
  abstract  = {Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and suggest that it is a direction worth pursuing to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length coding schemes.},
  file      = {2017 Generative Compression - Santurkar et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Image & Video\\2017 Generative Compression - Santurkar et al..pdf:application/pdf},
  langid    = {english},
  month     = {jun},
  year      = {2018},
}

@Article{2019LaudeComprehensive,
  author       = {Laude, Thorsten and Adhisantoso, Yeremia Gunawan and Voges, Jan and Munderloh, Marco and Ostermann, Jörn},
  date         = {2019},
  journaltitle = {{APSIPA} Transactions on Signal and Information Processing},
  title        = {A Comprehensive Video Codec Comparison},
  doi          = {10.1017/ATSIP.2019.23},
  issn         = {2048-7703},
  pages        = {e30},
  url          = {https://www.cambridge.org/core/product/identifier/S2048770319000234/type/journal_article},
  urldate      = {2021-03-03},
  volume       = {8},
  abstract     = {In this paper, we compare the video codecs {AV}1 (version 1.0.0-2242 from August 2019), {HEVC} ({HM} and x265), {AVC} (x264), the exploration software {JEM} which is based on {HEVC}, and the {VVC} (successor of {HEVC}) test model {VTM} (version 4.0 from February 2019) under two fair and balanced configurations: All Intra for the assessment of intra coding and Maximum Coding Efficiency with all codecs being tuned for their best coding efficiency settings. {VTM} achieves the highest coding efficiency in both configurations, followed by {JEM} and {AV}1. The worst coding efficiency is achieved by x264 and x265, even in the placebo preset for highest coding efficiency. {AV}1 gained a lot in terms of coding efficiency compared to previous versions and now outperforms {HM} by 24\% {BD}-Rate gains. {VTM} gains 5\% over {AV}1 in terms of {BD}-Rates. By reporting separate numbers for {JVET} and {AOM} test sequences, it is ensured that no bias in the test sequences exists. When comparing only intra coding tools, it is observed that the complexity increases exponentially for linearly increasing coding efficiency.},
  file         = {2019 A Comprehensive Video Codec Comparison - Laude et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Image & Video\\2019 A Comprehensive Video Codec Comparison - Laude et al..pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
  shortjournal = {{APSIPA} Transactions on Signal and Information Processing},
}

@Article{1999JiangImage,
  author       = {Jiang, J.},
  date         = {1999-07},
  journaltitle = {Signal Processing: Image Communication},
  title        = {Image compression with neural networks – A survey},
  doi          = {10.1016/S0923-5965(98)00041-1},
  issn         = {09235965},
  number       = {9},
  pages        = {737--760},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0923596598000411},
  urldate      = {2021-03-03},
  volume       = {14},
  abstract     = {Apart from the existing technology on image compression represented by series of {JPEG}, {MPEG} and H.26x standards, new technology such as neural networks and genetic algorithms are being developed to explore the future of image coding. Successful applications of neural networks to vector quantization have now become well established, and other aspects of neural network involvement in this area are stepping up to play signi"cant roles in assisting with those traditional technologies. This paper presents an extensive survey on the development of neural networks for image compression which covers three categories: direct image compression by neural networks; neural network implementation of existing techniques, and neural network based technology which provide improvement over traditional algorithms. 1999 Elsevier Science B.V. All rights reserved.},
  file         = {1999 Image compression with neural networks – A survey - Jiang.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Image & Video\\1999 Image compression with neural networks – A survey - Jiang.pdf:application/pdf},
  langid       = {english},
  shortjournal = {Signal Processing: Image Communication},
}

@Article{2014RehmanImage,
  author       = {Rehman, Mehwish and Sharif, Muhammad and Raza, Mudassar},
  date         = {2014-01-27},
  journaltitle = {Research Journal of Applied Sciences, Engineering and Technology},
  title        = {Image Compression: A Survey},
  doi          = {10.19026/rjaset.7.303},
  issn         = {20407459, 20407467},
  number       = {4},
  pages        = {656--672},
  url          = {http://maxwellsci.com/jp/mspabstract.php?jid=RJASET&doi=rjaset.7.303},
  urldate      = {2021-03-03},
  volume       = {7},
  abstract     = {Image Compression is a demanding field in this era of communication. There is a need to study and analyze the literature for image compression, as the demand for images, video sequences and computer animation has increased at very high rate so that the increment is drastically over the years. Multimedia data whether graphics, audio, video data which is uncompress requires considerable transmission bandwidth and storage capacity. So this leads to the need of compression of images and all multimedia applications to save storage and transmission time. In this study we discuss different compression algorithms used to reduce size of images without quality reduction.},
  file         = {2014 Image Compression A Survey - Rehman et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Image & Video\\2014 Image Compression A Survey - Rehman et al..pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
  shortjournal = {{RJASET}},
  shorttitle   = {Image Compression},
}

@InProceedings{2016TodericiVariable,
  author       = {Toderici, George and O'Malley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
  booktitle    = {International Conference on Learning Representations},
  date         = {2016-03-01},
  title        = {Variable Rate Image Compression with Recurrent Neural Networks},
  eprint       = {1511.06085},
  eprinttype   = {arxiv},
  url          = {http://arxiv.org/abs/1511.06085},
  urldate      = {2021-03-03},
  abstract     = {A large fraction of Internet trafﬁc is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will signiﬁcantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional {LSTM} recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efﬁcient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32×32 thumbnails, our {LSTM}-based approaches provide better visual quality than (headerless) {JPEG}, {JPEG}2000 and {WebP}, with a storage size that is reduced by 10\% or more.},
  file         = {2016 Variable Rate Image Compression with Recurrent Neural Networks - Toderici et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Image & Video\\2016 Variable Rate Image Compression with Recurrent Neural Networks - Toderici et al..pdf:application/pdf},
  journaltitle = {International Conference on Learning Representations},
  langid       = {english},
}

@InProceedings{2017BalleEnd,
  author     = {Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
  booktitle  = {International Conference on Learning Representations},
  date       = {2017-03-03},
  title      = {End-to-end Optimized Image Compression},
  eprint     = {1611.01704},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/1611.01704},
  urldate    = {2021-03-03},
  abstract   = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear ﬁlters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate–distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate–distortion curve, as speciﬁed by a trade-off parameter. Across an independent set of test images, we ﬁnd that the optimized method generally exhibits better rate–distortion performance than the standard {JPEG} and {JPEG} 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using {MS}-{SSIM}.},
  file       = {2017 End-to-end Optimized Image Compression - Ballé et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Image & Video\\2017 End-to-end Optimized Image Compression - Ballé et al..pdf:application/pdf},
  langid     = {english},
}

@Article{2017TheisLossy,
  author       = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Huszár, Ferenc},
  date         = {2017-03-01},
  journaltitle = {International Conference on Learning Representations},
  title        = {Lossy Image Compression with Compressive Autoencoders},
  eprint       = {1703.00395},
  eprinttype   = {arxiv},
  url          = {http://arxiv.org/abs/1703.00395},
  urldate      = {2021-03-03},
  abstract     = {We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more ﬂexible than existing codecs. Autoencoders have the potential to address this need, but are difﬁcult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufﬁcient to train deep autoencoders competitive with {JPEG} 2000 and outperforming recently proposed approaches based on {RNNs}. Our network is furthermore computationally efﬁcient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
  file         = {2017 Lossy Image Compression with Compressive Autoencoders - Theis et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Image & Video\\2017 Lossy Image Compression with Compressive Autoencoders - Theis et al..pdf:application/pdf},
  langid       = {english},
}

@Article{2015WangImage,
  author       = {Wang, Bo and Gao, Yubin},
  date         = {2015-03-01},
  journaltitle = {{TELKOMNIKA} (Telecommunication Computing Electronics and Control)},
  title        = {An Image Compression Scheme Based on Fuzzy Neural Network},
  doi          = {10.12928/telkomnika.v13i1.1270},
  issn         = {2302-9293, 1693-6930},
  number       = {1},
  pages        = {137},
  url          = {http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/1270},
  urldate      = {2021-03-03},
  volume       = {13},
  abstract     = {Image compression technology is to compress the redundancy between the pixels to reduce the transmission broadband and storage space by using the correlation of the image pixels. Fuzzy neural network effectively integrates neural network technology and fuzzy technology; combines learning, selfadaptivity, imagination and identity and uses rule-based reasoning and fuzzy information processing in the nodes; thus greatly improving the transparency of fuzzy neural network. This paper mainly investigates the applications of fuzzy neural network in image compression and realizes the image compression and reconstruction of fuzzy neural network. It is demonstrated in the simulation experiment that the image compression algorithm based on fuzzy neural network has significant advantages in training speed, compression quality and robustness.},
  file         = {2015 An Image Compression Scheme Based on Fuzzy Neural Network - Wang and Gao.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Image & Video\\2015 An Image Compression Scheme Based on Fuzzy Neural Network - Wang and Gao.pdf:application/pdf},
  journal      = {{TELKOMNIKA} (Telecommunication Computing Electronics and Control)},
  langid       = {english},
  month        = {mar},
  publisher    = {Universitas Ahmad Dahlan},
  shortjournal = {{TELKOMNIKA}},
  year         = {2015},
}

@InProceedings{2017TodericiFull,
  author    = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
  booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  date      = {2017-07-07},
  title     = {Full Resolution Image Compression with Recurrent Neural Networks},
  doi       = {10.1109/CVPR.2017.577},
  publisher = {{IEEE}},
  url       = {http://arxiv.org/abs/1608.05148},
  urldate   = {2021-03-03},
  abstract  = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network ({RNN})-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare {RNN} types ({LSTM}, associative {LSTM}) and introduce a new hybrid of {GRU} and {ResNet}. We also study “one-shot” versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3\%–8.8\% {AUC} (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the ﬁrst neural network architecture that is able to outperform {JPEG} at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
  file      = {2017 Full Resolution Image Compression with Recurrent Neural Networks - Toderici et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Image & Video\\2017 Full Resolution Image Compression with Recurrent Neural Networks - Toderici et al..pdf:application/pdf},
  langid    = {english},
  month     = {jul},
  year      = {2017},
}

@Article{2018JohnstonImproved,
  author    = {Johnston, Nick and Vincent, Damien and Minnen, David and Covell, Michele and Singh, Saurabh and Chinen, Troy and Hwang, Sung Jin and Shor, Joel and Toderici, George},
  date      = {2017-03-29},
  title     = {Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks},
  doi       = {10.1109/CVPR.2018.00461},
  eprint    = {1703.10114},
  url       = {http://arxiv.org/abs/1703.10114},
  urldate   = {2021-03-03},
  abstract  = {We propose a method for lossy image compression based on recurrent, convolutional neural networks that outperforms {BPG} (4:2:0), {WebP}, {JPEG}2000, and {JPEG} as measured by {MS}-{SSIM}. We introduce three improvements over previous research that lead to this state-of-the-art result. First, we show that training with a pixel-wise loss weighted by {SSIM} increases reconstruction quality according to several metrics. Second, we modify the recurrent architecture to improve spatial diffusion, which allows the network to more effectively capture and propagate image information through the network’s hidden state. Finally, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efﬁciently use the limited number of bits to encode visually complex image regions. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well recently published methods based on deep neural networks.},
  booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
  file      = {2017 Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks - Johnston et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Image & Video\\2017 Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks - Johnston et al..pdf:application/pdf},
  langid    = {english},
  month     = {jun},
  publisher = {{IEEE}},
  year      = {2018},
}

@InProceedings{2017GunawanInvestigation,
  author     = {Gunawan, Teddy Surya and Rashid, Siti Aisyah Abdul and Kartiwi, Mira},
  booktitle  = {2017 {IEEE} 4th International Conference on Smart Instrumentation, Measurement and Application ({ICSIMA})},
  date       = {2017-11},
  title      = {Investigation of various algorithms on multichannel audio compression},
  doi        = {10.1109/ICSIMA.2017.8311985},
  eventtitle = {2017 {IEEE} 4th International Conference on Smart Instrumentation, Measurement and Application ({ICSIMA})},
  isbn       = {978-1-5386-3960-3},
  location   = {Putrajaya},
  pages      = {1--5},
  publisher  = {{IEEE}},
  url        = {http://ieeexplore.ieee.org/document/8311985/},
  urldate    = {2021-03-03},
  file       = {2017 Investigation of various algorithms on multichannel audio compression - Gunawan et al.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Sound\\2017 Investigation of various algorithms on multichannel audio compression - Gunawan et al.pdf:application/pdf},
}

@InProceedings{2016GregorTowards,
  author    = {Gregor, Karol and Besse, Frederic and Jimenez Rezende, Danilo and Danihelka, Ivo and Wierstra, Daan},
  booktitle = {Advances in Neural Information Processing Systems},
  date      = {2016},
  title     = {Towards Conceptual Compression},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  pages     = {9},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2016/file/4abe17a1c80cbdd2aa241b70840879de-Paper.pdf},
  volume    = {29},
  abstract  = {We introduce convolutional {DRAW}, a homogeneous deep generative model achieving state-of-the-art performance in latent variable image modeling. The algorithm naturally stratiﬁes information into higher and lower level details, creating abstract features and as such addressing one of the fundamentally desired properties of representation learning. Furthermore, the hierarchical ordering of its latents creates the opportunity to selectively store global information about an image, yielding a high quality ‘conceptual compression’ framework.},
  file      = {Towards Conceptual Compression - Gregor et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\Towards Conceptual Compression - Gregor et al..pdf:application/pdf},
  langid    = {english},
}

@Article{2016AnuradhaDetailed,
  author       = {Anuradha, D. and Bhuvaneswari, S.},
  date         = {2016-03},
  journaltitle = {Annals of Data Science},
  title        = {A Detailed Review on the Prominent Compression Methods Used for Reducing the Data Volume of Big Data},
  doi          = {10.1007/s40745-016-0069-9},
  issn         = {2198-5804, 2198-5812},
  number       = {1},
  pages        = {47--62},
  url          = {http://link.springer.com/10.1007/s40745-016-0069-9},
  urldate      = {2021-03-03},
  volume       = {3},
  abstract     = {The volume of Big data is the primary challenge faced by today’s electronic world. Compressing data should be an important aspect of the huge volume to improve the overall performance of the Big data management system and Big data analytics. There is a quiet few compression methods that can reduce the cost of data management and data transfer and improve efﬁciency of data analysis. Adaptive data compression approach ﬁnds out the suitable data compression technique and the location of the data compression. De-duplication removes duplicate data from the Big data store. Resemblance detection and elimination algorithm uses two techniques namely, Dup-Adj and improved super-feature approach. Using them the similar data chunks are separated from non-similar data chunks. The Delta compression is also used to compress the data before storage. The general compression algorithms are computationally complex and also degrade the application response time. To address this application-speciﬁc {ZIP}-{IO} framework for {FPGA} accelerated compression is studied. In this framework a simple instruction trace entropy compression algorithm is implemented in {FPGA} substrate. The Record-aware Compression ({RaC}) technique guarantees that the splitting of compressed data blocks does not contain partial records in the data blocks and it is implemented in Hadoop {MapReduce}.},
  file         = {2016 A Detailed Review on the Prominent Compression Methods Used for Reducing the Data Volume of Big Data - Anuradha and Bhuvaneswari.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\2016 A Detailed Review on the Prominent Compression Methods Used for Reducing the Data Volume of Big Data - Anuradha and Bhuvaneswari.pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
  shortjournal = {Ann. Data. Sci.},
}

@Article{2019DeviKothaReview,
  author       = {Devi Kotha, Harika and Tummanapally, Madhumitha and Upadhyay, Vikash Kumar},
  date         = {2019-05},
  journaltitle = {Journal of Physics: Conference Series},
  title        = {Review on Lossless Compression Techniques},
  doi          = {10.1088/1742-6596/1228/1/012007},
  issn         = {1742-6588, 1742-6596},
  pages        = {012007},
  url          = {https://iopscience.iop.org/article/10.1088/1742-6596/1228/1/012007},
  urldate      = {2021-03-03},
  volume       = {1228},
  abstract     = {In the present world data compression is used in every field. Through data compression the bits required to represent a message will be reduced. By compressing the given data, we can save the storage capacity, files are transferred at high speed, storage hardware is decreased so that its cost is also decreased, and storage bandwidth is decreased. There are many methods to compress the data. But in this paper, we are discussing about Huffman coding and Arithmetic coding. For various input streams we are comparing adaptive Huffman coding and arithmetic coding and we will observe which technique will be more efficient to compress the data.},
  file         = {2019 Review on Lossless Compression Techniques - Devi Kotha et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\2019 Review on Lossless Compression Techniques - Devi Kotha et al..pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
  shortjournal = {J. Phys.: Conf. Ser.},
}

@Article{2021Jayasankarsurvey,
  author       = {Jayasankar, Uthayakumar and Thirumal, Vengattaraman and Ponnurangam, Dhavachelvan},
  date         = {2021-02},
  journaltitle = {Journal of King Saud University - Computer and Information Sciences},
  title        = {A survey on data compression techniques: From the perspective of data quality, coding schemes, data type and applications},
  doi          = {10.1016/j.jksuci.2018.05.006},
  issn         = {13191578},
  number       = {2},
  pages        = {119--140},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S1319157818301101},
  urldate      = {2021-03-03},
  volume       = {33},
  abstract     = {Explosive growth of data in digital world leads to the requirement of efﬁcient technique to store and transmit data. Due to limited resources, data compression ({DC}) techniques are proposed to minimize the size of data being stored or communicated. As {DC} concepts results to effective utilization of available storage area and communication bandwidth, numerous approaches were developed in several aspects. In order to analyze how {DC} techniques and its applications have evolved, a detailed survey on many existing {DC} techniques is carried out to address the current requirements in terms of data quality, coding schemes, type of data and applications. A comparative analysis is also performed to identify the contribution of reviewed techniques in terms of their characteristics, underlying concepts, experimental factors and limitations. Finally, this paper insight to various open issues and research directions to explore the promising areas for future developments.},
  file         = {2021 A survey on data compression techniques From the perspective of data quality, coding schemes, data type and applications - Jayasankar et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIB\\Articles\\Compression\\2021 A survey on data compression techniques From the perspective of data quality, coding schemes, data type and applications - Jayasankar et al..pdf:application/pdf},
  keywords     = {relevant},
  langid       = {english},
  relevance    = {relevant},
  shortjournal = {Journal of King Saud University - Computer and Information Sciences},
  shorttitle   = {A survey on data compression techniques},
}

@Article{Shannon_1948,
  author    = {C. E. Shannon},
  title     = {A Mathematical Theory of Communication},
  doi       = {10.1002/j.1538-7305.1948.tb01338.x},
  number    = {3},
  pages     = {379--423},
  volume    = {27},
  file      = {:C\:/Users/tesse/Desktop/Files/Dropbox/BIB/Articles/Compression/1948 A Mathematical Theory of Communication - C. E. Shannon.pdf:PDF},
  journal   = {Bell System Technical Journal},
  keywords  = {relevant},
  month     = {jul},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  relevance = {relevant},
  year      = {1948},
}

@Comment{jabref-meta: databaseType:biblatex;}
